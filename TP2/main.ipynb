{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitvenvf5e9d0eb35aa462ebf80aee9a8493d47",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TP2: Support Vector Machines\n",
    "\n",
    "*By Daniel Deutsch and Kevin Khul*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# Exercise 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_breastcancer(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "\n",
    "    # The column 0 isn't important for us here\n",
    "    y = data[:, 1]*2 - 1\n",
    "    X = data[:, 2:]\n",
    "\n",
    "    # Standardisation of the matrix\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    X = X / np.std(X, axis=0)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breastcancer(\"./datasets/wdbc_M1_B0.data\")"
   ]
  },
  {
   "source": [
    "# Exercise 2.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 2\n",
    "### 2.1\n",
    "\n",
    "From the proposed initial problem, we have (considering c = 1)\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "f(v, a, \\xi) = \\frac{1}{2} \\sum_j v_j^2 + \\sum_i \\xi_i\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Observing the above function, we see that it is a increasing with $\\xi_i$, $\\forall i$ and $v_j$, $\\forall j$.\n",
    "\n",
    "Looking at the constraints of the original problem, we see that $\\forall i$, $\\xi_i$ must be greater or equal to zero and also greater or equal to $1 - y_i(x_i^Tv + a)$.\n",
    "\n",
    "Therefore, we define a new $\\xi_min \\in \\mathbb{R}^n$ as \n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "\\xi_{min_i} = max(0, 1 - y_i(x_i^Tv + a))\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "It stores the minimum value of $\\xi_i, \\forall i$.\n",
    "\n",
    "Therefore, upon the considerations of a increasing function made above\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "f(v, a, \\xi) \\ge f(v, a, \\xi_{min}).\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "And, therefore\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "min_{v,a,\\xi} f(v,a,\\xi) \\ge min_{v,a,\\xi} f(v, a, \\xi_{min}) = min_{v,a} f(v, a, \\xi_{min})\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Besides, if we consider a function $g(x,y)$ which accepts a minimum, $\\forall y_0 \\in Y$ we have\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "min_{x\\in X, y \\in Y} g(x,y) \\le min_{x \\in X} g(x,y_0)\n",
    "\\end{aligned}\n",
    "\n",
    "Analogally,\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "min_{v,a,\\xi} f(v,a,\\xi) \\le min_{v,a} f(v, a, \\xi_{min})\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Using both inequalities, we can finally write\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "min_{v,a,\\xi} f(v,a,\\xi) = min_{v,a} f(v, a, \\xi_{min})\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Which proves the equivalence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 2.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's plot the function and the straight lines in between both existing gradients."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(z):\n",
    "    return np.maximum(np.zeros(z.shape), 1-z)\n",
    "\n",
    "z = np.arange(-10, 10, 0.01)\n",
    "\n",
    "plt.plot(z, h(z), color='blue', linewidth=3.0)\n",
    "for j in range(21):\n",
    "    i = 1\n",
    "    s = -j/20\n",
    "    y = (z-1)*s\n",
    "    plt.plot(z, y, color='green', linewidth=0.3)\n",
    "    \n",
    "plt.legend([\"h(z)\", \"Possible tangent\"])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We see that h(z) is derivable for all z different from 1. Therefore, if $z \\neq$ 1\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\partial h(z) = \\frac{dh(z)}{dz}\n",
    "\\end{aligned}\n",
    "\n",
    "For z = 1, we have (by looking at the above plot) that all the slopes are in between -1 and 0, therefore \n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\partial h(z) = \n",
    "    \\begin{cases}\n",
    "        \\{-1\\} & if \\ z \\lt 1 \\\\\n",
    "        [-1;0] & if \\ z = 1 \\\\\n",
    "        \\{0\\} & if \\ z \\gt 1 \\\\\n",
    "    \\end{cases}\n",
    "\\end{aligned}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 2.3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We first rewrite $f(v,a)$:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    f(v, a) = \\frac{1}{2}\\cdotp\\sum_{j=1}^{m} v^2_j + c\\cdotp\\sum_{i=1}^{n}max(0,1-y_i(x_i^Tv+a))\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Our goal is to find N and H separable such that we can write:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    f(v, a) = N(v, a) + c\\cdotp H(M(v, a))\n",
    "\\end{aligned}\n",
    "\n",
    "$N(v,a)$ is relatively easy to find, as we can write : \n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    N(v, a) = \\sum_{j=1}^{m} h_j $ with $ h_j(v) = \\frac{1}{2}v_j^2\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Noting that it is, by construction, a separable function.\n",
    "\n",
    "Now, when searching for $H(v, a)$ and $M(v, a)$, we know that\n",
    "\n",
    "<br>\n",
    "\\begin{aligned}\n",
    "    H(v, a)\\cdotp M(v, a) = \\sum_{i=1}^{n}max(0, 1-y_i(x_i^Tv+a))\n",
    "\\end{aligned}\n",
    "<br>\n",
    "\n",
    "To build M, we take a diagonal matrix D with all the elements $y_i$\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    D = \\begin{pmatrix}\n",
    "    y_1 & ... & 0 \\\\\n",
    "    ... & ... & ...\\\\\n",
    "    0 & ... & y_n\n",
    "    \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Then, we create another matrix L, composed by the observations $x_i^T$ and complemented with a column of ones.\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    L= \\begin{pmatrix}\n",
    "    x_1^T & 1 \\\\\n",
    "    ... & ...\\\\\n",
    "    x_n^T & 1\n",
    "    \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, we can write M as $M = D\\cdotp L$, and\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "M(v, a) = M \\cdotp \\begin{pmatrix} v\\\\a\\end{pmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Also we write H as $H = \\sum_{i=1}^{n}h_i $, where $ h_i(W) = max(0, 1-w_i)$  for vector a $W$. Note that H is, also by construction, separable and W is the product of $M$ and $\\begin{pmatrix}v\\\\a\\end{pmatrix}$, yielding $M(v, a)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 2.4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.insert(X,X.shape[1],1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return randomly the subgradient of h taken at a given point\n",
    "def H_subgradient(z):\n",
    "    return ((z<1)*(-1)+(np.random.rand(1)[0]-1)*(z==1))\n",
    "\n",
    "# Defining function f from the matrices and transformations found above\n",
    "def f(v, a): \n",
    "    v_sum = 1/2*sum([v[j]**2 for j in range(len(v))])\n",
    "    \n",
    "    # Matrix D\n",
    "    D = np.diag(y)\n",
    "    \n",
    "    # Matrix M\n",
    "    M = np.dot(X.transpose(), D)\n",
    "    \n",
    "    # Matrix M(v,a)\n",
    "    M_v_a = np.dot(M.transpose(), np.concatenate((v, [a]), axis=0))\n",
    "    \n",
    "    # Finally computing the second term\n",
    "    c_sum = sum([max(0,1-M_v_a[i]) for i in range(len(M_v_a))])\n",
    "    \n",
    "    # Adding with the first term and returning\n",
    "    return v_sum + c_sum\n",
    "\n",
    "# Finally computing the gradient of f(v,a)\n",
    "def subgradient(v, a):\n",
    "    # Compute the subgradient of the first part of the function\n",
    "    N_subgrad = np.concatenate((v,[0]),axis=0)\n",
    "    \n",
    "    # Defining matrices like in previous function \n",
    "    D = np.diag(y)\n",
    "    M = np.dot(D, X)\n",
    "    M_v_a = np.dot(M,np.concatenate((v, [a]), axis=0))\n",
    "    \n",
    "    # Use previous defined function to find the subgradient of the second part of the function f\n",
    "    H_subgrad = np.array([H_subgradient(z) for z in M_v_a])\n",
    "    \n",
    "    # Multiplying it by the matrix M (transpose)\n",
    "    multiplied_H_subgrad = np.dot(M.T, H_subgrad)\n",
    "    \n",
    "    # Returning the final result for the subgradient of f(v,a)\n",
    "    return (N_subgrad + multiplied_H_subgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.ones(30)\n",
    "f(v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgradient(v, 1)"
   ]
  },
  {
   "source": [
    "# Exercise 2.5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The subgradient function\n",
    "def subgradient_method(v0,a0,epsilon=1,itmax=10000):\n",
    "    gk = subgradient(v0,a0)\n",
    "    xk = np.zeros(31)\n",
    "    itr = 0\n",
    "    while(np.linalg.norm(gk) > epsilon and itr <= itmax): \n",
    "        xk -= 1/(itr+1)*gk\n",
    "        gk = subgradient(xk[:-1],xk[-1])\n",
    "        print(f\"\\rCurrent iteration: {itr+1}/{itmax}\", end=\"\")\n",
    "        itr += 1\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = np.zeros(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = subgradient_method(v0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The subgradient method gives a mininumum of: {f(x_min[:-1], x_min[-1]):.2f}, given the initial conditions (v0, a0) = 0\")"
   ]
  },
  {
   "source": [
    "# Exercise 3.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have that:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    E[f_I (v, a)] \\quad & = \\quad \\sum_{i = 1}^n P(I = i) \\ f_i (v, a) \\\\\n",
    "    & = \\quad \\frac{1}{n} \\sum_{i = 1}^n f_i (v, a) \\\\\n",
    "    & = \\quad \\frac{1}{n} \\sum_{i = 1}^n \\left(c \\ n \\ max(0, \\ 1-y_i(x_i^T v + a)) + \\frac{1}{2} \\sum_{j = 1}^m v_j^2 \\right) \\\\\n",
    "    & = \\quad \\frac{1}{n} \\sum_{i = 1}^n (c \\ n \\ max(0, \\ 1-y_i(x_i^T v + a))) + \\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{2} \\sum_{j = 1}^m v_j^2 \\\\\n",
    "    & = \\quad \\sum_{i = 1}^n (c \\ max(0, \\ 1-y_i(x_i^T v + a))) + \\frac{1}{2} \\sum_{j = 1}^m v_j^2 \\\\\n",
    "    & = \\quad f(v, a)\n",
    "\\end{aligned}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 3.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let $M_i$ be the $i_th$ row of $M$, i.e. $M_i = y_i(x_i^T, 1)$. This way, we have:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    M_i\\begin{pmatrix} v \\\\ a \\end{pmatrix} \\quad = \\quad y_i(x_i^Tv + a)\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Therefore, we can write $f_i(v, a) = N(v, a) + c \\ n \\ h(M_i(v,a))$, which implies $\\partial f_i(v, a) = (v, 0) + c \\ n \\ M_i^T \\ \\partial h(M_i(v, a))$. Thus, we have:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\partial f_i(v, a) =\n",
    "    \\begin{cases}\n",
    "        (v, 0) - n \\ c \\ M_i^T & if \\ M_i(v, a) \\lt 1 \\\\\n",
    "        (v, 0) + t \\ n \\ c \\ M_i^t, \\quad t \\in [-1; 0] & if \\ M_i(v, a) = 1 \\\\\n",
    "        (v, 0) & if \\ M_i(v, a) \\gt 1\n",
    "    \\end{cases}\n",
    "\\end{aligned}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 3.3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_h_M_i(va, i):\n",
    "    return ((np.dot(M[i,:],va)>=1)- 1) * M[i,:]\n",
    "\n",
    "def delta_f_i(va, i):\n",
    "    return delta_N(va) + c * M.shape[0] * delta_h_M_i(va,i)\n",
    "\n",
    "def stoch_grad_method(va0, N):\n",
    "    n = M.shape[0]\n",
    "    va_moy = np.zeros(va0.shape)\n",
    "    gamma_sum = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        I = np.random.randint(n)\n",
    "        gamma = 0.001/np.sqrt(i+1)\n",
    "        gamma_sum += gamma\n",
    "        va_moy += va0 * gamma\n",
    "        \n",
    "        va0 = va0 - gamma * delta_f_i(va0,I)\n",
    "    \n",
    "    #return va0\n",
    "    return va_moy/gamma_sum\n",
    "\n",
    "def stochastic_subgradient_method(M, itmax=10000):\n",
    "    pass"
   ]
  },
  {
   "source": [
    "# Exercise 4.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The Lagrangian function $L()$ associated to Problem (1) is:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 4.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have $g(x, \\phi) = - \\frac{1}{2\\rho} \\phi^2 + \\frac{\\rho}{2} (max(0, \\ x + \\rho^{-1} \\phi))^2$ with $\\rho > 0$. Firstly, to find $\\nabla_x g(x, \\phi)$, we do:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\nabla_x g(x, \\phi) \\quad = \\quad \\frac{\\partial g(x, \\phi)}{\\partial x} \\quad & = \\quad \\frac{\\partial \\left(-\\frac{1}{2\\rho} \\phi^2\\right)}{\\partial x} + \\frac{\\partial \\left(\\frac{\\rho}{2} (max(0, \\ x + \\rho^{-1} \\phi))^2\\right)}{\\partial x} \\\\\n",
    "    & = \\quad 0 + \\frac{2 \\rho}{2} (max(0, \\ x + \\rho^{-1} \\phi)) \\\\\n",
    "    & = \\quad \\rho \\ max(0, \\ x + \\rho^{-1} \\phi)\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, to find $\\nabla_\\phi g(x, \\phi)$:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\nabla_\\phi g(x, \\phi) \\quad = \\quad \\frac{\\partial g(x, \\phi)}{\\partial \\phi} \\quad & = \\quad \\frac{\\partial \\left(-\\frac{1}{2\\rho} \\phi^2\\right)}{\\partial \\phi} + \\frac{\\partial \\left(\\frac{\\rho}{2} (max(0, \\ x + \\rho^{-1} \\phi))^2\\right)}{\\partial \\phi} \\\\\n",
    "    & = \\quad -\\frac{2}{2\\rho} \\phi + \\frac{2 \\rho}{2 \\rho} (max(0, \\ x +\\rho^{-1} \\phi)) \\\\\n",
    "    & = \\quad -\\frac{\\phi}{\\rho} + max(0, \\ x + \\rho^{-1} \\phi) \\\\\n",
    "    & = \\quad max(-\\rho^{-1} \\phi, \\ x)\n",
    "\\end{aligned}\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 4.3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From *Exercise 4.2* we know that $\\frac{\\partial g(x, \\phi)}{\\partial x} = \\rho \\ max(0, \\ x + \\rho^{-1} \\phi)$ and that $\\frac{\\partial g(x, \\phi)}{\\partial \\phi} = max(-\\rho^{-1} \\phi, \\ x)$. \n",
    "\n",
    "To categorize the function $x \\rightarrow g(x, \\phi)$ as concave or convexe, all we need to do is verify the signal of $\\frac{\\partial^2 g(x, \\phi)}{\\partial x^2}$:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial^2 g(x, \\phi)}{\\partial x^2} \\quad & = \\quad \\frac{\\partial (\\rho \\ max(0, \\ x + \\rho^{-1} \\phi))}{\\partial x} \\\\\n",
    "    & = \\quad \\rho \\ max(0, 1) \\\\\n",
    "    & = \\quad \\rho\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Since by definition we have that $\\rho > 0$, we confirm that the function $x \\rightarrow g(x, \\phi)$ is convex.\n",
    "\n",
    "Similarly, for the function $\\phi \\rightarrow g(x, \\phi)$, we analyse the signal of $\\frac{\\partial^2 g(x, \\phi)}{\\partial \\phi^2}$:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial^2 g(x, \\phi)}{\\partial \\phi^2} \\quad & = \\quad \\frac{\\partial (max(-\\rho^{-1} \\phi, \\ x))}{\\partial \\phi} \\\\\n",
    "    & = \\quad \\ max(-\\rho^{-1}, 0) \\\\\n",
    "    & = \\quad 0\n",
    "\\end{aligned}\n",
    "\n",
    "<br>\n",
    "\n",
    "Since the Hessian of $\\phi \\rightarrow g(x, \\phi)$ is $\\leq 0$, the function is concave."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise 4.4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_gradient():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "# Exercise 4.5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangian_gradient():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "# Exercise 4.6"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_lagrangian():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "# Exercise 5.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}